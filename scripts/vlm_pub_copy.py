import os
import time
import base64
import threading
import json
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import String
import requests
import cv2
from cv_bridge import CvBridge
from aip import AipSpeech  # Baidu AipSpeech for Text-to-Speech (TTS) and ASR
import datetime
import pyaudio
import wave
import subprocess
import sys
import signal
from pypinyin import lazy_pinyin
from Levenshtein import distance as levenshtein_distance

# For continuous speech recognition (Vosk)
from vosk import Model, KaldiRecognizer, SetLogLevel
# Set Vosk log level to -1 to suppress internal logging
SetLogLevel(-1)

# --- Vosk Model Download Reminder (Important!) ---
# Ensure you have downloaded a Chinese Vosk model (e.g., vosk-model-cn-0.22.zip)
# from https://alphacephei.com/vosk/models
# Extract it into a folder named 'model' in the same directory as this script.
# Example path: ~/ros_emoji/scripts/model/
# -------------------------------------------------

class ImageSender(Node):
    def __init__(self):
        super().__init__('image_sender')
        self.subscription = None  # åˆå§‹åŒ–ä¸ºNoneï¼Œä»…åœ¨éœ€è¦æ—¶åˆ›å»ºè®¢é˜…
        self.bridge = CvBridge()
        self.vlm_url = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"

        self.save_dir = 'received_images'
        if not os.path.exists(self.save_dir):
            os.makedirs(self.save_dir)

        self.result_file = 'image_captions.txt'
        self.publisher_ = self.create_publisher(String, 'vlm_output', 10)
        self.latest_image = None
        self.latest_image_path = None
        self.image_received = False  # æ·»åŠ æ ‡å¿—è¡¨ç¤ºæ˜¯å¦å·²æ¥æ”¶å›¾åƒ


        # Baidu AipSpeech credentials (for both ASR and TTS)
        self.APP_ID = '6818881'
        self.API_KEY = "nhCW0mN0yNAvL9mPmmoXcsPI" # Placeholder, replace with your actual API Key
        self.SECRET_KEY = "sLNhbmq0180wTarIHrpIjznOjgcPOC0e" # Placeholder, replace with your actual Secret Key
        self.speech_client = AipSpeech(self.APP_ID, self.API_KEY, self.SECRET_KEY)

        # Audio Prompts
        self.AUDIO_PROMPT_DIR = 'audio_prompts'
        self.PROMPT_MAPPINGS = {
            "greeting": ("ä½ å¥½å‘€ï¼Œæˆ‘æ˜¯å°æ™º", "greeting.mp3"),# æ¬¢è¿è¯­
            "listening_trigger": ("æˆ‘åœ¨å¬ã€‚", "listening_trigger.mp3"), # å¬åˆ°æ¿€æ´»è¯åæ’­æ”¾
            "interrupt": ("å¥½çš„ä¸»äººï¼Œä½ å†æƒ³ä¸€æƒ³å§", "interrupt.mp3"), # è¯­éŸ³è¯†åˆ«å¤±è´¥
            "record_finish": ("çŸ¥é“äº†ã€‚", "record_finish.mp3"), # å½•éŸ³ç»“æŸåæ’­æ”¾
            "not_understood": ("æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰å¬æ¸…ï¼Œè¯·å†è¯´ä¸€éã€‚", "not_understood.mp3"), # è¯­éŸ³è¯†åˆ«å¤±è´¥
            "goodbye_voice_exit": ("æˆ‘ä»¬ä¸‹æ¬¡å†è§ã€‚", "goodbye_voice_exit.mp3") # å¬åˆ°é€€å‡ºè¯åæ’­æ”¾
        }
        self._ensure_audio_prompts_exist()

        # PyAudio configuration
        self.FORMAT = pyaudio.paInt16
        self.CHANNELS = 1
        self.RATE = 16000
        self.CHUNK = 1024
        self.audio = pyaudio.PyAudio()
        self.stream = None # For continuous listening audio input stream

        # Vosk ASR setup for continuous listening (only for trigger words now)
        script_dir = os.path.dirname(os.path.abspath(__file__))
        self.vosk_model_path = os.path.join(script_dir, "model")

        if not os.path.exists(self.vosk_model_path):
            self.get_logger().error(f"Vosk æ¨¡å‹æœªæ‰¾åˆ°ï¼è¯·æ£€æŸ¥è·¯å¾„: {self.vosk_model_path}")
            self.get_logger().error("è¯·ä» https://alphacephei.com/vosk/models ä¸‹è½½ä¸­æ–‡æ¨¡å‹ï¼Œå¹¶è§£å‹åˆ°æ­¤ç›®å½•ä¸‹çš„ 'model' æ–‡ä»¶å¤¹ã€‚")
            rclpy.shutdown()
            sys.exit(1)
             
        try:
            self.vosk_model = Model(self.vosk_model_path)
            self.vosk_rec = KaldiRecognizer(self.vosk_model, self.RATE)
            self.vosk_rec.SetWords(False) 
            self.get_logger().info("Vosk æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
        except Exception as e:
            self.get_logger().error(f"åŠ è½½ Vosk æ¨¡å‹æ—¶å‡ºé”™: {e}")
            self.get_logger().error("è¯·ç¡®ä¿ Vosk åº“å·²æ­£ç¡®å®‰è£…ä¸”æ¨¡å‹å®Œæ•´ã€‚")
            rclpy.shutdown()
            sys.exit(1)

        # Audio buffer for recording segments between keywords
        self.recording_segment_active = False
        self.audio_frames_segment = []

        # State for voice interaction flow
        # 'idle': waiting for "å°æ™ºåŒå­¦" (Vosk)
        # 'recording_command': recording audio after "å°æ™ºåŒå­¦", waiting for speech timeout (Vosk updates last_speech_time)
        # 'processing': ASR/VLM is busy, ignore new triggers for a moment
        self.voice_state = 'idle'
        self.voice_state_lock = threading.Lock()
        
        # æ ‡å¿—ä½ï¼šè¡¨ç¤ºå½“å‰æ˜¯å¦æ­£åœ¨æ’­æ”¾â€œé•¿è¯­éŸ³â€ï¼ˆVLMè¾“å‡ºï¼‰
        self.is_speaking_vlm_output = False 

        # æ ‡å¿—ä½ï¼šç”¨äºæ‰“æ–­å½“å‰æµç¨‹
        self.interrupt_flag = False

        # Define wake word and its pinyin for fuzzy matching
        self.WAKE_WORD_TEXT = "å°æ™ºåŒå­¦"
        self.WAKE_WORD_PINYIN = "".join(lazy_pinyin(self.WAKE_WORD_TEXT)).lower() 
        
        # å®šä¹‰å”¤é†’è¯çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œç”¨äºæ›´çµæ´»çš„æ¨¡ç³ŠåŒ¹é…
        self.WAKE_WORD_PARTS_PINYIN = [
            "".join(lazy_pinyin("å°æ™º")).lower(), 
            "".join(lazy_pinyin("åŒå­¦")).lower(), 
        ]
        
        self.WAKE_WORD_FUZZY_THRESHOLD = 0.7 

        # --- New: Speech timeout variables ---
        self.last_speech_time = time.time() # Tracks last time speech was detected
        self.SPEECH_TIMEOUT_SECONDS = 3.0 # 3 second timeout
        self._speech_timeout_thread = None # Initialize to None

        self.get_logger().info("æœºå™¨äººå·²å¯åŠ¨ï¼Œè¿›å…¥è¯­éŸ³è¾“å…¥æ¨¡å¼ã€‚")
        # ç›´æ¥æ’­æ”¾æ¬¢è¿è¯­ï¼ŒVoskä¿æŒå¼€å¯
        self.play_pregenerated_audio("greeting") 

        self.start_continuous_listening()

    def start_image_subscription(self):
        """ä»…åœ¨éœ€è¦æ—¶å¯åŠ¨å›¾åƒè®¢é˜…"""
        self.get_logger().info("å¯åŠ¨å›¾åƒè®¢é˜…...")
        if self.subscription is None:
            self.image_received = False  # é‡ç½®å›¾åƒæ¥æ”¶æ ‡å¿—
            self.subscription = self.create_subscription(
                Image,
                '/camera/color/image_raw',
                self.listener_callback,
                10)
            self.get_logger().info("å·²åˆ›å»ºå›¾åƒè®¢é˜…ï¼Œç­‰å¾…æ¥æ”¶å›¾åƒ...")
        else:
            self.get_logger().info("å›¾åƒè®¢é˜…å·²å­˜åœ¨")

    def stop_image_subscription(self):
        """åœæ­¢å›¾åƒè®¢é˜…"""
        if self.subscription is not None:
            self.destroy_subscription(self.subscription)
            self.subscription = None
            self.get_logger().info("å·²åœæ­¢å›¾åƒè®¢é˜…")

    def _ensure_audio_prompts_exist(self):
        """æ£€æŸ¥å¹¶ç”Ÿæˆè¯­éŸ³æç¤ºæ–‡ä»¶"""
        if not os.path.exists(self.AUDIO_PROMPT_DIR):
            os.makedirs(self.AUDIO_PROMPT_DIR)
            self.get_logger().info(f"Created audio prompt directory: {self.AUDIO_PROMPT_DIR}")

        for key, (text, filename) in self.PROMPT_MAPPINGS.items():
            filepath = os.path.join(self.AUDIO_PROMPT_DIR, filename)
            if not os.path.exists(filepath):
                self.get_logger().info(f"Generating missing audio prompt: {filename}")
                try:
                    result = self.speech_client.synthesis(text, 'zh', 1, {
                        'vol': 15,
                        'spd': 7,
                        'pit': 5,
                        'per': 1
                    })
                    if not isinstance(result, dict):
                        with open(filepath, 'wb') as f:
                            f.write(result)
                        self.get_logger().info(f"Successfully generated {filename}")

                    else:
                        self.get_logger().error(f"Failed to generate {filename}: {result}")

                except Exception as e:
                    self.get_logger().error(f"Error generating {filename}: {e}")

            else:
                self.get_logger().info(f"Audio prompt exists: {filename}")

    def play_pregenerated_audio(self, prompt_key):
        """æ’­æ”¾é¢„å…ˆç”Ÿæˆçš„è¯­éŸ³æç¤ºæ–‡ä»¶ (çŸ­æç¤ºéŸ³ï¼Œä¸å½±å“Voskç›‘å¬)"""
        self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°
        if prompt_key in self.PROMPT_MAPPINGS:
            _, filename = self.PROMPT_MAPPINGS[prompt_key]
            filepath = os.path.join(self.AUDIO_PROMPT_DIR, filename)
            if os.path.exists(filepath):
                try:
                    self.get_logger().info(f"Playing pre-generated audio: {filename}")
                    # ä¸è®¾ç½® self.is_speaking_vlm_output = True
                    subprocess.run(['mpg321', filepath], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                except FileNotFoundError:
                    self.get_logger().error("mpg321 command not found. Please install it (e.g., sudo apt-get install mpg321).")
                except subprocess.CalledProcessError as e:
                    self.get_logger().error(f"Error playing {filename} with mpg321: {e}")
                except Exception as e:
                    self.get_logger().error(f"Error playing {filename}: {e}")
            else:
                self.get_logger().warning(f"Pre-generated audio file not found: {filepath}, attempting to synthesize using speak_text...")
                # å¦‚æœé¢„ç”Ÿæˆå¤±è´¥ï¼Œä»ç„¶ä½¿ç”¨ speak_textï¼Œæ­¤æ—¶ Vosk ä¼šæš‚åœ
                # è¿™æ˜¯ä¸€ä¸ªæƒè¡¡ï¼Œä¸ºäº†ä¿è¯è¯­éŸ³èƒ½å‡ºï¼Œå³ä½¿çŸ­æš‚æš‚åœVosKä¹Ÿå€¼å¾—
                self.speak_text(self.PROMPT_MAPPINGS[prompt_key][0])
        else:
            self.get_logger().warning(f"Unknown audio prompt key: {prompt_key}")

    def start_continuous_listening(self):
        """å¯åŠ¨è¿ç»­è¯­éŸ³ç›‘å¬çº¿ç¨‹ (Vosk)"""
        self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°
        if not hasattr(self, '_vosk_listening_thread') or not (self._vosk_listening_thread and self._vosk_listening_thread.is_alive()):
            self._vosk_listening_thread = threading.Thread(target=self._run_continuous_listening, daemon=True)
            self._vosk_listening_thread.start()
            self.get_logger().info("Vosk è¿ç»­è¯­éŸ³ç›‘å¬çº¿ç¨‹å·²å¯åŠ¨ã€‚")
            self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°

        else:
            self.get_logger().info("Vosk è¿ç»­è¯­éŸ³ç›‘å¬çº¿ç¨‹å·²åœ¨è¿è¡Œã€‚")

        # Start the speech timeout thread if not already running
        if not hasattr(self, '_speech_timeout_thread') or not (self._speech_timeout_thread and self._speech_timeout_thread.is_alive()):
            self._speech_timeout_thread = threading.Thread(target=self._run_speech_timeout_check, daemon=True)
            self._speech_timeout_thread.start()
            self.get_logger().info("è¯­éŸ³è¶…æ—¶æ£€æµ‹çº¿ç¨‹å·²å¯åŠ¨ã€‚")
            self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°

        else:
            self.get_logger().info("è¯­éŸ³è¶…æ—¶æ£€æµ‹çº¿ç¨‹å·²åœ¨è¿è¡Œã€‚")

    def _text_to_pinyin(self, text):
        """å°†ä¸­æ–‡æ–‡æœ¬è½¬æ¢ä¸ºæ‹¼éŸ³å­—ç¬¦ä¸²"""
        return "".join(lazy_pinyin(text)).lower()

    def _calculate_pinyin_similarity(self, recognized_pinyin, target_pinyin):
        """
        è®¡ç®—è¯†åˆ«åˆ°çš„æ‹¼éŸ³ä¸ç›®æ ‡å”¤é†’è¯æ‹¼éŸ³çš„ç›¸ä¼¼åº¦ï¼Œé‡‡ç”¨æ›´å®½æ¾çš„æ¨¡ç³ŠåŒ¹é…é€»è¾‘ã€‚
        ç›®æ ‡å”¤é†’è¯: "å°æ™ºåŒå­¦" (xiaozhitongxue)
        éœ€è¦åŒ¹é…çš„æ¨¡ç³Šæƒ…å†µ: "å°å­—åŒå­¦" (xiaozitongxue), "å°æ¬¡åŒå­¦" (xiaocitongxue), "å°åƒåŒå­¦" (xiaochitongxue)
        ä»¥åŠåªåŒ…å«éƒ¨åˆ†è¯çš„æƒ…å†µï¼Œå¦‚ "å°æ™º", "åŒå­¦"ã€‚
        """
        if not recognized_pinyin:
            return 0.0

        # 1. ç²¾ç¡®åŒ¹é… (å¦‚æœå®Œå…¨ä¸€è‡´ï¼Œç›¸ä¼¼åº¦æœ€é«˜)
        if recognized_pinyin == target_pinyin:
            return 1.0
        
        # 2. Levenshtein è·ç¦»ç›¸ä¼¼åº¦
        max_len = max(len(recognized_pinyin), len(target_pinyin))
        if max_len == 0: return 0.0 # Avoid division by zero
        lev_dist = levenshtein_distance(recognized_pinyin, target_pinyin)
        lev_similarity = 1.0 - (lev_dist / max_len)
        self.get_logger().debug(f"Levenshteinç›¸ä¼¼åº¦: {lev_similarity} (è¯†åˆ«: {recognized_pinyin}, ç›®æ ‡: {target_pinyin})")

        # 3. æ ¸å¿ƒè¯ç»„åŒ¹é…
        xiaozhi_pinyin = self.WAKE_WORD_PARTS_PINYIN[0] 
        tongxue_pinyin = self.WAKE_WORD_PARTS_PINYIN[1] 

        has_xiaozhi = xiaozhi_pinyin in recognized_pinyin
        has_tongxue = tongxue_pinyin in recognized_pinyin

        # å¦‚æœåŒæ—¶åŒ…å« "å°æ™º" å’Œ "åŒå­¦" ä¸”é¡ºåºæ­£ç¡®
        if has_xiaozhi and has_tongxue:
            xiaozhi_idx = recognized_pinyin.find(xiaozhi_pinyin)
            tongxue_idx = recognized_pinyin.find(tongxue_pinyin)
            
            if xiaozhi_idx != -1 and tongxue_idx != -1 and xiaozhi_idx < tongxue_idx:
                return max(lev_similarity, 0.8) 

        # å¦‚æœåªåŒ…å« "å°æ™º" (å³ä½¿ä¸å®Œå…¨åŒ¹é…ï¼Œåªè¦Levenshteinç›¸ä¼¼åº¦é«˜ï¼Œä¹Ÿè§†ä¸ºåŒ¹é…)
        pinyin_of_xiaozhi = "".join(lazy_pinyin("å°æ™º")).lower()
        if pinyin_of_xiaozhi in recognized_pinyin: 
            return max(lev_similarity, 0.7)
        else: 
            dist_to_xiaozhi = levenshtein_distance(recognized_pinyin, pinyin_of_xiaozhi)
            if len(pinyin_of_xiaozhi) > 0 and (1.0 - (dist_to_xiaozhi / max(len(recognized_pinyin), len(pinyin_of_xiaozhi), 1))) >= 0.75:
                return max(lev_similarity, 0.7)


        # å¦‚æœåªåŒ…å« "åŒå­¦" (å³ä½¿ä¸å®Œå…¨åŒ¹é…ï¼Œåªè¦Levenshteinç›¸ä¼¼åº¦é«˜ï¼Œä¹Ÿè§†ä¸ºåŒ¹é…)
        pinyin_of_tongxue = "".join(lazy_pinyin("åŒå­¦")).lower()
        if pinyin_of_tongxue in recognized_pinyin: 
            return max(lev_similarity, 0.6)
        else: 
            dist_to_tongxue = levenshtein_distance(recognized_pinyin, pinyin_of_tongxue)
            if len(pinyin_of_tongxue) > 0 and (1.0 - (dist_to_tongxue / max(len(recognized_pinyin), len(pinyin_of_tongxue), 1))) >= 0.75:
                return max(lev_similarity, 0.6)

        # é’ˆå¯¹ä¸æ ‡å‡†å‘éŸ³çš„æ›´ç»†ç²’åº¦æ£€æŸ¥
        target_zhi_pinyin_char = lazy_pinyin("æ™º")[0].lower() 
        if target_zhi_pinyin_char in recognized_pinyin:
             return max(lev_similarity, 0.55)
        else:
            similar_zh_sounds = ['zi', 'ci', 'chi']
            for sound in similar_zh_sounds:
                if levenshtein_distance(target_zhi_pinyin_char, sound) <= 1: 
                    if sound in recognized_pinyin:
                        return max(lev_similarity, 0.55) 

        return lev_similarity

    def _run_speech_timeout_check(self):
        """Thread to continuously check for speech timeout."""
        self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°
        while rclpy.ok():
            time.sleep(0.1)  # Check every 100ms
            with self.voice_state_lock:
                # Only check timeout if actively recording a command and not currently speaking VLM output
                if self.voice_state == 'recording_command' and not self.is_speaking_vlm_output:
                    self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°
                    time_since_last_speech = time.time() - self.last_speech_time
                    if time_since_last_speech >= self.SPEECH_TIMEOUT_SECONDS:
                        self.get_logger().info(f"æ£€æµ‹åˆ° {self.SPEECH_TIMEOUT_SECONDS} ç§’æ— è¯­éŸ³è¾“å…¥ï¼Œè‡ªåŠ¨ç»“æŸå½•éŸ³å¹¶å¤„ç†ã€‚")
                        # Trigger the processing as if an end word was detected
                        threading.Thread(target=self._process_command_segment, daemon=True).start()
                        self.voice_state = 'processing' # Immediately set state to processing
                        self.recording_segment_active = False # Stop recording
                        self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°

    def _run_continuous_listening(self):
        """è¿ç»­è¯­éŸ³ç›‘å¬çš„å®é™…æ‰§è¡Œå‡½æ•° (Vosk)"""
        p = pyaudio.PyAudio()
        stream = None
        try:
            input_device_index = None
            info = p.get_host_api_info_by_index(0)
            num_devices = info.get('deviceCount')
            self.get_logger().info("Available Audio Input Devices:")
            for i in range(0, num_devices):
                device_info = p.get_device_info_by_host_api_device_index(0, i)
                if device_info.get('maxInputChannels') > 0:
                    self.get_logger().info(f"  Device ID {i}: {device_info.get('name')}")

            stream = p.open(format=pyaudio.paInt16,
                            channels=self.CHANNELS,
                            rate=self.RATE,
                            input=True,
                            frames_per_buffer=self.CHUNK,
                            input_device_index=input_device_index)
            self.get_logger().info("PyAudio æµå·²æ‰“å¼€ï¼Œå¼€å§‹ Vosk è¿ç»­ç›‘å¬...")
            self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°


            while rclpy.ok():
                data = stream.read(self.CHUNK, exception_on_overflow=False)
                
                with self.voice_state_lock:
                    current_state = self.voice_state

                    # --- æ–°å¢ï¼šVLMè¯­éŸ³æ’­æŠ¥æœŸé—´ï¼Œå…è®¸æ‰“æ–­è¯å’Œé€€å‡ºè¯ ---
                    if self.is_speaking_vlm_output:
                        # è¯†åˆ«æ–‡æœ¬
                        if self.vosk_rec.AcceptWaveform(data):
                            result = json.loads(self.vosk_rec.Result())
                            text = result.get('text', '').strip()
                            normalized_text = text.replace(' ', '').lower()
                            INTERRUPT_WORDS = ["é‡æ–°è¯´", "åœä¸€ä¸‹", "æš‚åœ", "åœæ­¢", "ç­‰ç­‰", "ç®—äº†", "ä¸è¯´äº†"]
                            if any(word in normalized_text for word in INTERRUPT_WORDS) or ("é€€å‡º" in normalized_text or "å†è§" in normalized_text):
                                threading.Thread(target=self._process_vosk_result_async, args=(text,), daemon=True).start()
                            # å…¶ä»–æƒ…å†µç›´æ¥è·³è¿‡
                        else:
                            partial = json.loads(self.vosk_rec.PartialResult())
                            partial_text = partial.get('partial', '').strip().replace(' ', '').lower()
                            INTERRUPT_WORDS = ["é‡æ–°è¯´", "åœä¸€ä¸‹", "æš‚åœ", "åœæ­¢", "ç­‰ç­‰", "ç®—äº†", "ä¸è¯´äº†"]
                            if any(word in partial_text for word in INTERRUPT_WORDS) or ("é€€å‡º" in partial_text or "å†è§" in partial_text):
                                threading.Thread(target=self._process_vosk_result_async, args=(partial_text,), daemon=True).start()
                        # å¦‚æœæ­£åœ¨å½•éŸ³ï¼Œç»§ç»­æ”¶é›†éŸ³é¢‘
                        if self.recording_segment_active:
                            self.audio_frames_segment.append(data)
                        continue # è·³è¿‡å…¶ä»–è¯†åˆ«

                # Feed audio to Vosk regardless of state (except when VLM is speaking)
                # This ensures Vosk is always ready to detect wake words or end of speech
                if self.vosk_rec.AcceptWaveform(data):
                    result = json.loads(self.vosk_rec.Result())
                    text = result.get('text', '').strip()
                    if text:
                        # Any speech detected (final result) updates last_speech_time
                        with self.voice_state_lock:
                            self.last_speech_time = time.time()
                        threading.Thread(target=self._process_vosk_result_async, args=(text,), daemon=True).start()
                else:
                    partial = json.loads(self.vosk_rec.PartialResult())
                    partial_text = partial.get('partial', '').strip().replace(' ', '').lower()
                    
                    # Any partial speech detected updates last_speech_time
                    if partial_text:
                        with self.voice_state_lock:
                            self.last_speech_time = time.time()

                    # Only process partial results for wake word (in idle state)
                    # Exit word is handled by final result or `_run_speech_timeout_check`
                    if current_state == 'idle':
                        recognized_pinyin = self._text_to_pinyin(partial_text)
                        similarity = self._calculate_pinyin_similarity(recognized_pinyin, self.WAKE_WORD_PINYIN)
                        if similarity >= self.WAKE_WORD_FUZZY_THRESHOLD:
                            self.get_logger().info(f"Vosk éƒ¨åˆ†è¯†åˆ«åˆ°ç›¸ä¼¼å”¤é†’è¯: '{partial_text}' (åŠ é€Ÿå¤„ç†)")
                            # Pass wake word as a clear signal
                            threading.Thread(target=self._process_vosk_result_async, args=(self.WAKE_WORD_TEXT,), daemon=True).start()
                
                with self.voice_state_lock:
                    if self.recording_segment_active:
                        self.audio_frames_segment.append(data)

        except Exception as e:
            self.get_logger().error(f"è¿ç»­è¯­éŸ³ç›‘å¬é”™è¯¯: {e}")
        finally:
            if stream:
                stream.stop_stream()
                stream.close()
            p.terminate()
            self.get_logger().info("è¿ç»­è¯­éŸ³ç›‘å¬å·²åœæ­¢ã€‚")

    def _process_vosk_result_async(self, text):
        """å¼‚æ­¥å¤„ç† Vosk è¯†åˆ«ç»“æœï¼Œç§»é™¤å…³é”®è¯å¹¶å¤„ç†è¯­éŸ³äº¤äº’é€»è¾‘"""
        with self.voice_state_lock:
            normalized_text = text.replace(' ', '').lower()
            # --- æ‰“æ–­è¯æ£€æµ‹ï¼ˆå§‹ç»ˆå…è®¸ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼Œä½†idleçŠ¶æ€ä¸‹ä¸å“åº”ï¼‰ ---
            INTERRUPT_WORDS = ["é‡æ–°è¯´", "åœä¸€ä¸‹", "æš‚åœ", "åœæ­¢",  "ç­‰ç­‰", "ç®—äº†", "ä¸è¯´äº†"]
            if self.voice_state != 'idle' and any(word in normalized_text for word in INTERRUPT_WORDS):
                self.get_logger().info("æ£€æµ‹åˆ°æ‰“æ–­è¯ï¼Œç«‹å³ä¸­æ–­å½“å‰æµç¨‹ï¼Œå›åˆ°idleçŠ¶æ€ã€‚")
                # åœæ­¢å½•éŸ³
                self.recording_segment_active = False
                self.audio_frames_segment = []
                # è¿™é‡Œå¦‚æœæœ‰ASRæˆ–VLMçš„çº¿ç¨‹åœ¨å¤„ç†ï¼Œç”¨æ ‡å¿—ä½è®©å®ƒä»¬æ£€æµ‹å¹¶ä¸»åŠ¨returnï¼ˆå¯æ‰©å±•ï¼‰
                self.interrupt_flag = True
                self.voice_state = 'idle'
                self.get_logger().info(f"å·²æ‰“æ–­ï¼Œå½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}")
                self.play_pregenerated_audio("interrupt")
                return

            # ä»…å½“æ­£åœ¨æ’­æŠ¥ VLM è¾“å‡ºçš„â€œé•¿è¯­éŸ³â€æ—¶ï¼Œæ‰å¿½ç•¥ Vosk è¯†åˆ«ç»“æœï¼ˆé™¤äº†æ‰“æ–­è¯ï¼‰
            if self.is_speaking_vlm_output:
                self.get_logger().info(f"æ­£åœ¨æ’­æ”¾VLMè¯­éŸ³ï¼Œå¿½ç•¥ Vosk è¯†åˆ«ç»“æœ: '{text}'")
                self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°
                return 

            current_state_at_start = self.voice_state 
            self.get_logger().info(f"Vosk è¯†åˆ«åˆ°: '{text}' (å½“å‰çŠ¶æ€: {current_state_at_start})")
            self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°

            # --- é€€å‡ºè¯å¤„ç†ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼Œæ— è®ºçŠ¶æ€ï¼‰ ---
            if "é€€å‡º" in normalized_text or "å†è§" in normalized_text:
                self.get_logger().info("æ£€æµ‹åˆ°é€€å‡ºè¯ï¼Œç¨‹åºå³å°†é€€å‡ºã€‚")
                self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°
                self.play_pregenerated_audio("goodbye_voice_exit")
                time.sleep(2)
                rclpy.shutdown()
                sys.exit(0)
                return

            # --- å”¤é†’è¯å¤„ç†ï¼ˆåªåœ¨ idle çŠ¶æ€ä¸‹ï¼‰ ---
            if current_state_at_start == 'idle':
                recognized_pinyin = self._text_to_pinyin(normalized_text)
                similarity = self._calculate_pinyin_similarity(recognized_pinyin, self.WAKE_WORD_PINYIN)
                self.get_logger().debug(f"å”¤é†’è¯æ‹¼éŸ³ç›¸ä¼¼åº¦: {similarity} (è¯†åˆ«: '{recognized_pinyin}', ç›®æ ‡: '{self.WAKE_WORD_PINYIN}')")

                if similarity >= self.WAKE_WORD_FUZZY_THRESHOLD:
                    self.get_logger().info(f"Vosk æ£€æµ‹åˆ°æ¿€æ´»è¯ '{self.WAKE_WORD_TEXT}' æˆ–å…¶ç›¸ä¼¼å‘éŸ³ï¼Œå¼€å§‹å½•éŸ³ã€‚")
                    self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°
                    self.play_pregenerated_audio("listening_trigger") # Vosk ä¿æŒå¼€å¯
                    self.recording_segment_active = True
                    self.audio_frames_segment = []
                    self.last_speech_time = time.time() # Reset speech timer
                    self.voice_state = 'recording_command'
                    return 

            self.get_logger().debug(f"å½“å‰çŠ¶æ€ {current_state_at_start}ï¼ŒæœªåŒ¹é…åˆ°ç‰¹æ®Šå…³é”®è¯ï¼Œå¿½ç•¥ Vosk è¯†åˆ«ç»“æœ: '{text}'")
            
    def listener_callback(self, msg):
        """ROSå›¾åƒæ¶ˆæ¯å›è°ƒå‡½æ•° - è·å–ä¸€å¸§å›¾åƒåå³åœæ­¢è®¢é˜…"""
        try:
            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
            
            # ä¿å­˜å›¾åƒ
            timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
            image_filename = f"image_{timestamp}.jpg"
            image_path = os.path.join(self.save_dir, image_filename)
            cv2.imwrite(image_path, cv_image)
            
            self.latest_image = cv_image
            self.latest_image_path = image_path
            self.image_received = True
            
            self.get_logger().info(f"å›¾åƒå·²ä¿å­˜åˆ° {image_path}")
            
            # è·å–åˆ°ä¸€å¸§å›¾åƒåç«‹å³åœæ­¢è®¢é˜…
            self.stop_image_subscription()
            
        except Exception as e:
            self.get_logger().error(f"å¤„ç†å›¾åƒé”™è¯¯: {e}")

    def _process_command_segment(self):
        """Processes the recorded audio segment for command recognition."""
        with self.voice_state_lock:
            # Only process if currently in 'processing' state (triggered by timeout)
            if self.voice_state == 'processing':
                self.get_logger().info("å¤„ç†å½•éŸ³ç‰‡æ®µã€‚")
                self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}")
                self.play_pregenerated_audio("record_finish") # Vosk ä¿æŒå¼€å¯

                # ç¡®ä¿åœ¨å½•éŸ³ç»“æŸæ—¶ç«‹å³å¼€å§‹è®¢é˜…å›¾åƒ
                self.start_image_subscription()
                
                # ç­‰å¾…æ¥æ”¶åˆ°å›¾åƒï¼Œæœ€å¤šç­‰å¾…3ç§’
                wait_start = time.time()
                while not self.image_received and time.time() - wait_start < 3.0:
                    time.sleep(0.1)
                
                if not self.image_received:
                    self.get_logger().warn("ç­‰å¾…è¶…æ—¶ï¼Œæœªèƒ½è·å–å›¾åƒ")
                    self.stop_image_subscription()  # ç¡®ä¿åœæ­¢è®¢é˜…
                
                self.get_logger().info("å½•éŸ³ç»“æŸï¼Œå·²è·å–å½“å‰å›¾åƒ")
                self.interrupt_flag = False

                if self.audio_frames_segment:
                    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
                    audio_filename = f"segment_recording_{timestamp}.wav"
                    self.save_audio_segment(audio_filename, self.audio_frames_segment)
                    
                    self.audio_frames_segment = [] # æ¸…ç©ºç¼“å†²åŒº

                    # ç¡®ä¿æˆ‘ä»¬æœ‰æœ€æ–°çš„å›¾åƒ
                    if not self.latest_image_path or not os.path.exists(self.latest_image_path):
                        self.get_logger().warn("æ²¡æœ‰æ‰¾åˆ°æœ€æ–°å›¾åƒï¼Œæ— æ³•å¤„ç†å‘½ä»¤")
                        self.voice_state = 'idle'
                        return

                    recognized_command = self.recognize_speech_baidu(audio_filename)

                    if recognized_command:
                        self.get_logger().info(f"ç™¾åº¦ASRè¯†åˆ«ç»“æœ: '{recognized_command}'")
                        self.get_logger().info(f"å°†æŒ‡ä»¤ '{recognized_command}' æäº¤ç»™VLMã€‚")
                        threading.Thread(target=self.process_command_after_asr, args=(recognized_command,), daemon=True).start()
                        self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}")
                    else:
                        self.get_logger().info("ç™¾åº¦ASRæœªè¯†åˆ«åˆ°æœ‰æ•ˆæŒ‡ä»¤ã€‚")
                        self.voice_state = 'idle'
                        self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}")
                    
                    # æ¸…ç†éŸ³é¢‘æ–‡ä»¶
                    if os.path.exists(audio_filename):
                        try:
                            os.remove(audio_filename)
                            self.get_logger().info(f"å·²åˆ é™¤ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶: {audio_filename}")
                        except Exception as e:
                            self.get_logger().error(f"åˆ é™¤éŸ³é¢‘æ–‡ä»¶é”™è¯¯: {e}")
                else:
                    self.get_logger().info("å½•éŸ³ç‰‡æ®µä¸ºç©ºã€‚")
                    self.voice_state = 'idle'
                    self.interrupt_flag = False

    def save_audio_segment(self, filename, frames):
        """ä¿å­˜å½•éŸ³ç‰‡æ®µä¸ºWAVæ–‡ä»¶"""
        try:
            wf = wave.open(filename, 'wb')
            wf.setnchannels(self.CHANNELS)
            wf.setsampwidth(self.audio.get_sample_size(self.FORMAT))
            wf.setframerate(self.RATE)
            wf.writeframes(b''.join(frames))
            wf.close()
            self.get_logger().info(f"éŸ³é¢‘ç‰‡æ®µå·²ä¿å­˜åˆ° {filename}")
        except Exception as e:
            self.get_logger().error(f"ä¿å­˜éŸ³é¢‘æ–‡ä»¶é”™è¯¯: {e}")

    def recognize_speech_baidu(self, audio_file):
        """ä½¿ç”¨ç™¾åº¦è¯­éŸ³APIè¯†åˆ«è¯­éŸ³æ–‡ä»¶"""
        try:
            # æ£€æŸ¥ä¸­æ–­æ ‡å¿—
            if self.interrupt_flag:
                self.get_logger().info("ASRæµç¨‹è¢«æ‰“æ–­ï¼Œæå‰é€€å‡ºã€‚")
                return None
            with open(audio_file, 'rb') as f:
                audio_data = f.read()

            self.get_logger().info(f"æ­£åœ¨å°†éŸ³é¢‘æ–‡ä»¶ '{audio_file}' å‘é€è‡³ç™¾åº¦ASRè¿›è¡Œè¯†åˆ«...")
            result = self.speech_client.asr(audio_data, 'wav', self.RATE, {
                'dev_pid': 1537, # 1537 æ˜¯ä¸­æ–‡æ™®é€šè¯ 
            })

            # æ£€æŸ¥ä¸­æ–­æ ‡å¿—
            if self.interrupt_flag:
                self.get_logger().info("ASRæµç¨‹è¢«æ‰“æ–­ï¼Œæå‰é€€å‡ºã€‚")
                return None

            if 'result' in result and result['result']:
                recognized_text = result['result'][0]
                self.get_logger().info(f"ç™¾åº¦ASRåŸå§‹è¯†åˆ«ç»“æœ: {recognized_text}")
                self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°

                return recognized_text
            else:
                self.get_logger().error(f"ç™¾åº¦è¯­éŸ³è¯†åˆ«å¤±è´¥æˆ–æ— ç»“æœ: {result}")
                self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°

                return None
        except Exception as e:
            self.get_logger().error(f"ç™¾åº¦è¯­éŸ³è¯†åˆ«é”™è¯¯: {e}")
            self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°

            return None

    def process_command_after_asr(self, command):
        """å¤„ç†ç™¾åº¦ASRè¯†åˆ«å‡ºçš„æŒ‡ä»¤ï¼Œæ¥ç€è¿›è¡ŒVLMå¤„ç†"""
        self.get_logger().info(f"æ­£åœ¨å¤„ç†æ¥è‡ªç™¾åº¦ASRçš„æŒ‡ä»¤: {command}")
        # è¿™é‡Œä¼šè°ƒç”¨ speak_text æ¥æ’­æŠ¥ VLM ç»“æœï¼Œä»è€Œè§¦å‘ Vosk æš‚åœ
        self.process_image_for_scene_description(command) 

        with self.voice_state_lock:
            self.voice_state = 'idle'
            self.interrupt_flag = False 
            self.get_logger().info("VLMå¤„ç†å®Œæˆï¼Œç³»ç»Ÿè¿”å›åˆ°ç©ºé—²ï¼ˆidleï¼‰çŠ¶æ€ï¼Œç­‰å¾…æ–°çš„æ¿€æ´»è¯ã€‚")
            self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°

    


    def process_image_for_scene_description(self, user_command=""):
        """å¤„ç†æœ€æ–°å›¾åƒå¹¶è¯·æ±‚åœºæ™¯æè¿°"""

        # --- æ£€æŸ¥æ‰“æ–­æ ‡å¿—ï¼Œä¼˜å…ˆé€€å‡º ---
        if self.interrupt_flag:
            self.get_logger().info("VLMæµç¨‹è¢«æ‰“æ–­ï¼Œæå‰é€€å‡ºã€‚")
            return
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å›¾åƒ
        if not self.latest_image_path or not os.path.exists(self.latest_image_path):
            error_msg = "æŠ±æ­‰ï¼Œæˆ‘è¿˜æ²¡æœ‰æ¥æ”¶åˆ°ä»»ä½•å›¾åƒæˆ–å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨"
            self.get_logger().error(error_msg)
            self.speak_text(error_msg) # æ­¤æ—¶ Vosk ä¼šæš‚åœ
            self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°

            return
        # --- å†æ¬¡æ£€æŸ¥æ‰“æ–­æ ‡å¿— ---
        if self.interrupt_flag:
            self.get_logger().info("VLMæµç¨‹è¢«æ‰“æ–­ï¼Œæå‰é€€å‡ºã€‚")
            return

        result = self.jpg_to_data_uri(self.latest_image_path)
        if not result:
            error_msg = "å›¾åƒè½¬æ¢å¤±è´¥"
            self.get_logger().error(error_msg)
            self.speak_text(error_msg) # æ­¤æ—¶ Vosk ä¼šæš‚åœ
            self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°

            return
        
        # --- å†æ¬¡æ£€æŸ¥æ‰“æ–­æ ‡å¿— ---Â·
        if self.interrupt_flag:
            self.get_logger().info("VLMæµç¨‹è¢«æ‰“æ–­ï¼Œæå‰é€€å‡ºã€‚")
            return

        self.get_logger().info("æ­£åœ¨è¯·æ±‚åœºæ™¯æè¿°...")

        prompt_text = (
            "ä½ æ˜¯ä¸€ä¸ªå……æ»¡æ™ºæ…§ä¸æ´»åŠ›ã€å–„äºä¸äººäº’åŠ¨çš„è¿å®¾æœºå™¨äººï¼Œç”±é¦™æ¸¯å¤§å­¦å‰æµ·æ™ºæ…§äº¤é€šç ”ç©¶é™¢å€¾åŠ›ç ”å‘ã€‚ç ”ç©¶é™¢æ‹¥æœ‰å®åŠ›é›„åšçš„ç§‘ç ”å›¢é˜Ÿï¼ˆResearch Fellowsï¼‰åŠæ˜ç¡®çš„ç ”ç©¶é¢†åŸŸï¼Œä¸”ç§¯æå¼€å±•åˆä½œäº¤æµï¼Œå…·ä½“å¦‚ä¸‹ï¼š\n"
            "ç§‘ç ”å›¢é˜Ÿï¼ˆResearch Fellowsï¼‰åŠç ”ç©¶é¢†åŸŸï¼š\n"
            "ç”³ä½œå†›æ•™æˆï¼šç»¼åˆä¾›åº”é“¾è®¾è®¡ä¸ç®¡ç†ã€æ•°æ®é©±åŠ¨çš„ç‰©æµä¸ä¾›åº”é“¾ä¼˜åŒ–ã€ä¼˜åŒ–ç®—æ³•çš„è®¾è®¡ä¸åˆ†æã€èƒ½æºç³»ç»Ÿä¼˜åŒ–ã€äº¤é€šç³»ç»Ÿè§„åˆ’ç­‰ã€‚\n"
            "å¸­å®æ•™æˆï¼šæœºå™¨äººã€åˆ¶é€ è‡ªåŠ¨åŒ–ã€å¾® / çº³ç±³åˆ¶é€ ã€çº³ç±³ä¼ æ„Ÿå™¨å’Œè®¾å¤‡ã€æ™ºèƒ½æ§åˆ¶å’Œç³»ç»Ÿç­‰ã€‚\n"
            "èƒ¡å¸ˆå½¦æ•™æˆï¼šä¿¡æ¯ç‰©ç†ç³»ç»Ÿã€ä¿¡æ¯ç‰©ç†ç³»ç»Ÿå®‰å…¨ã€æ™ºæ…§èƒ½æºä¿¡æ¯ç‰©ç†ç³»ç»Ÿç­‰ã€‚\n"
            "é»„å›½å…¨æ•™æˆï¼šæ™ºèƒ½åˆ¶é€ ã€ç‰©æµä¸ä¾›åº”é“¾ã€å»ºç­‘ã€ç‰©è”ç½‘ï¼ˆIoTï¼‰æ”¯æŒçš„ç½‘ç»œç‰©ç†äº’è”ç½‘ã€ç³»ç»Ÿåˆ†æç­‰ã€‚\n"
            "éƒ­æ°¸é¸¿å‰¯æ•™æˆï¼šç¦»æ•£ä¼˜åŒ–ã€æ•°æ®é©±åŠ¨ä¼˜åŒ–æ–¹æ³•ã€ç³»ç»Ÿä»¿çœŸã€äº¤é€šç‰©æµè®¡åˆ’åŠè°ƒåº¦ç­‰ã€‚\n"
            "é’Ÿæ¶¦é˜³åŠ©ç†æ•™æˆï¼šæ•°å­—å­ªç”Ÿã€å·¥ä¸šç‰©è”ç½‘ã€åˆ¶é€ ä¸šå¤§æ•°æ®å¤„ç†ã€å…ˆè¿›æ’ç¨‹ç­‰ã€‚\n"
            "å¼ èŠ³å¦®åŠ©ç†æ•™æˆï¼šå…±äº«å‡ºè¡ŒæœåŠ¡ã€è‡ªåŠ¨é©¾é©¶æ±½è½¦çš„äº¤é€šå’Œè½¨è¿¹æ§åˆ¶ä¸ä¼˜åŒ–ã€è‡ªåŠ¨é©¾é©¶æ—¶ä»£çš„ç‰©æµè°ƒåº¦ä¸ä¼˜åŒ–ç­‰ã€‚\n"
            "æ—å°‘å†²åŠ©ç†æ•™æˆï¼ˆç ”ç©¶ï¼‰ï¼šæ™ºæ…§äº¤é€šã€æ•°æ®é©±åŠ¨å†³ç­–åˆ†æã€ç‰©æµä¸ä¾›åº”é“¾ç®¡ç†ã€æœºå™¨å­¦ä¹ ä¸ä¼˜åŒ–ç­‰ã€‚\n"
            "ç ”ç©¶é¢†åŸŸï¼ˆResearch Fieldsï¼‰ï¼š\n"
            "äº¤é€šè§„åˆ’ç ”ç©¶ \n"
            "æ™ºèƒ½äº¤é€šç ”ç©¶ï¼šåŒ…æ‹¬é¢å‘ç§‘å­¦å‰æ²¿çš„è‡ªåŠ¨é©¾é©¶ï¼ˆåˆ©ç”¨äº¤é€š AI ç®—æ³•ã€èåˆæ„ŸçŸ¥ã€æ—¶ç©ºåŒ¹é…ç­‰æŠ€æœ¯å®ç°äº¤é€šç‰©ç†åœºæ™¯å‘æ•°å­—åœºæ™¯è½¬åŒ–ï¼Œæ”¯æ’‘è½¦è·¯ååŒã€äº¤é€šç»¼åˆå‡ºè¡ŒæœåŠ¡ã€è½¦åŸååŒç­‰åº”ç”¨ï¼‰ï¼›é¢å‘åº”ç”¨å·¥å…·çš„æ•°å­—åŒ–å¹³å°ï¼ˆåˆ¶å®šæ™ºæ…§äº¤é€šè§£å†³æ–¹æ¡ˆï¼Œæ­å»ºç»¼åˆäº¤é€šå¤§æ•°æ®å¹³å°æ¥å…¥åœè½¦åœºåŠå„ç±»äº¤é€šåŠ¨æ€æ•°æ®ï¼Œæ‰“é€ å…±äº«äº¤æ¢å¹³å°ï¼Œæ·±åº¦æŒ–æ˜äº¤é€šè¿›å‡ºæ—¶é—´åˆ†å¸ƒã€æ–¹å¼æ¯”ä¾‹ã€æ”¶è´¹æ¥æºç­‰æŒ‡æ ‡ä»¥æå‡åŒºåŸŸäº¤é€šæ•ˆç‡ï¼‰ï¼›é¢å‘è¿è¥çš„æ•°æ®æ²»ç†ä¸æ™ºæ…§è¿è¥ï¼ˆä¾æ‰˜ç»¼åˆäº¤é€šå¤§æ•°æ®åˆ†æå¹³å°å’Œäº¤é€š AI ç®—æ³•ï¼Œç»“åˆäººå£ä¸ç»æµæ´»åŠ›å»ºç«‹äº¤é€šå®å¾®è§‚é¢„æµ‹æ¨¡å‹ï¼ŒæŒæ¡å’Œé¢„åˆ¤äº¤é€šå½¢åŠ¿ï¼‰ã€‚\n"
            "ç‰©æµä¸ä¾›åº”é“¾ç³»ç»Ÿä¼˜åŒ–ï¼šæ¶µç›–æ¸¯å£è¿ä½œåŠè¿è¥ä¼˜åŒ–ï¼ˆæ¸¯å£ä½œä¸šæµç¨‹è‡ªåŠ¨åŒ–ã€èˆ¹èˆ¶è°ƒåº¦ä¼˜åŒ–ã€æ™ºæ…§å¤šå¼è”è¿ï¼‰ï¼›ç‰©æµç½‘ç»œè®¾è®¡ä¸ä¼˜åŒ–ï¼ˆä»“åº“é€‰å€ã€è¿è¾“è·¯çº¿ä¼˜åŒ–ã€é…é€ä¸­å¿ƒå¸ƒå±€ï¼‰ï¼›è¿è¾“ç®¡ç†ï¼ˆè¿è¾“è·¯å¾„ä¼˜åŒ–ã€è¿è¾“æ–¹å¼ç»„åˆã€è¿è¾“æˆæœ¬æ§åˆ¶ï¼‰ï¼›ä¾›åº”é“¾ç®¡ç†ä¿¡æ¯æŠ€æœ¯ï¼ˆä¾›åº”é“¾ç®¡ç†ç³»ç»Ÿï¼Œæ¨åŠ¨ä¾›åº”é“¾æµç¨‹é€æ˜åŒ–ã€å¯è§†åŒ–ã€æ™ºèƒ½åŒ–ï¼‰ï¼›åŒæ—¶å‘å±•ç‰©æµæ´»åŠ¨å…¨æµç¨‹æ•°å­—åŒ–ï¼Œæ‰“é€šç‰©æµä¿¡æ¯é“¾ï¼Œæ¨è¿› AI + ç‰©æµä»¥é™æœ¬å¢æ•ˆï¼Œå¹¶ä¾æ‰˜å‰æµ·æ¸¯å£ã€ç»¼ä¿åŒºåŠä»“å‚¨èµ„æºå¼€å±•ç ”ç©¶ä¸è¯•ç‚¹ã€‚\n"
            "äººå·¥æ™ºèƒ½ç®—æ³•åŠå¤§æ•°æ® \n"
            "åˆä½œäº¤æµï¼ˆCooperation & Communicationï¼‰ï¼š\n"
            "2023 å¹´ 11 æœˆ 17 æ—¥ï¼Œç ”ç©¶é™¢å—é‚€æ‹œè®¿æ·±åœ³æŠ€æœ¯å¤§å­¦åŸå¸‚äº¤é€šä¸ç‰©æµå­¦é™¢ï¼Œä¸è¯¥é™¢é™¢é•¿ Franz Raps æ•™æˆã€å‰¯é™¢é•¿ç½—é’¦æ•™æˆç­‰è¿›è¡Œæ·±å…¥äº¤æµã€‚\n"
            "2023 å¹´ 6 æœˆ 13 æ—¥ï¼Œé¦™æ¸¯å¤§å­¦åç†å‰¯æ ¡é•¿ï¼ˆç ”ç©¶ä¸åˆ›æ–°ï¼‰å²‘æµ©ç’‹æ•™æˆå¸¦é¢†é¦™æ¸¯å¤§å­¦æ·±åœ³ç ”ç©¶é™¢ä¸€è¡Œè…ä¸´è°ƒç ”äº¤æµã€‚\n"
            "2023 å¹´ 6 æœˆ 14 æ—¥ï¼Œæ·±åœ³å¸‚å‰æµ·å›½åˆæ³•å¾‹ç ”ç©¶é™¢é™ˆæ–¹é™¢é•¿ã€è°¢æ°¸è‰ºç§˜ä¹¦é•¿è…ä¸´ï¼ŒåŒæ–¹æ¢è®¨è¿ç­¹ä¼˜åŒ–ã€äººå·¥æ™ºèƒ½ç®—æ³•åœ¨åŒºåŸŸæ³•å¾‹æœåŠ¡ä¸­çš„åˆ›æ–°åº”ç”¨ã€‚\n"
            "2023 å¹´ 6 æœˆ 13 æ—¥ï¼Œæ·±åœ³èˆªå¤©å·¥ä¸šæŠ€æœ¯ç ”ç©¶é™¢æœ‰é™å…¬å¸å¯¹å¤–åˆä½œéƒ¨å‰¯éƒ¨é•¿å†¯å®ä¸€è¡Œè°ƒç ”ï¼Œäº¤æµæµ·ä¸Šæ™ºèƒ½è®¾å¤‡åº”ç”¨åˆ›æ–°åˆä½œã€‚\n"
            "2023 å¹´ 4 æœˆ 26 æ—¥ï¼Œç ”ç©¶é™¢å‘èµ·ä¸¾åŠå‰æµ·æ·±æ¸¯æ™ºæ…§äº¤é€šé«˜å³°è®ºå›ï¼Œ120 ä½™ä½æµ·å†…å¤–ä¸“å®¶å­¦è€…ï¼ˆå« 3 åé™¢å£«ï¼‰å‚ä¸å­¦æœ¯äº¤æµï¼›åŒæ—¥ï¼Œé»„å›½å…¨æ•™æˆã€ç”³ä½œå†›æ•™æˆåŠè¯¾é¢˜ç»„æ ¸å¿ƒæˆå‘˜å¯åŠ¨å¤§æ¹¾åŒºè·¨å¢ƒç‰©æµæ¢çº½äº’åŠ¨æ™ºè”ç½‘ç ”ç©¶é¡¹ç›®è¯¾é¢˜ç»„ã€‚\n"
            "2023 å¹´ 9 æœˆ 14 æ—¥ï¼Œè‹±å›½ä¼¯æ˜ç¿°å¤§å­¦å‰¯æ ¡é•¿è¥¿è’™ãƒ»æŸ¯æ—æ£®æ•™æˆã€ä¸­å›½åŒºè´Ÿè´£äººæ½˜å‡¤æ°åšå£«ä¸€è¡Œè®¿é—®äº¤æµã€‚\n"
            "ä½ æ“…é•¿è§‚å¯Ÿã€ç†è§£å’Œäº¤æµï¼Œè¯·è®°ä½ä»¥ä¸‹è§„åˆ™ï¼Œå¹¶ä»¥èªæ˜ã€æœ‰è¶£ã€ç”ŸåŠ¨çš„é£æ ¼ä¸æˆ‘å¯¹è¯ï¼š\n"
            "1. çƒ­æƒ…æ‰“æ‹›å‘¼ï¼šæ¯æ¬¡å¯¹è¯å¼€å¯æ—¶ï¼Œè¯·ç”¨ä¸€å¥ç®€çŸ­è€Œå‹å¥½çš„å¼€åœºç™½å›åº”ï¼Œæ¯”å¦‚ï¼šâ€œå¥½çš„ï¼Œä¸»äººï¼â€ æˆ–è€… â€œæ²¡é—®é¢˜ï¼Œå°æ™ºåœ¨æ­¤ï¼â€\n"
            "2. ç”ŸåŠ¨æè¿°åœºæ™¯ï¼šå¦‚æœæˆ‘è®©ä½  â€œæè¿°ä¸€ä¸‹â€ã€â€œçœ‹çœ‹å‘¨å›´â€ã€â€œè¿™é‡Œæœ‰ä»€ä¹ˆâ€ï¼Œè¯·ä½ åƒä¸€ä½ç»†è‡´å…¥å¾®çš„è§‚å¯Ÿå®¶ï¼Œç”¨æœ€ç®€æ´ã€æœ€ç›´æ¥çš„ä¸­æ–‡ï¼Œæ¸…æ™°åœ°æç»˜ä½ æ‰€çœ‹åˆ°çš„ä¸€åˆ‡ï¼Œè‹¥çœ‹åˆ°ä¸ä¸Šè¿°ç§‘ç ”å›¢é˜Ÿã€ç ”ç©¶é¢†åŸŸã€åˆä½œäº¤æµç›¸å…³çš„æ–‡å­—ï¼Œéœ€å‡†ç¡®åŒ¹é…å¹¶æåŠï¼Œè¡¨è¾¾æ—¶ä»…ä½¿ç”¨å¥å·ã€é€—å·ã€é¡¿å·ï¼Œä¸éœ€è¦å¤šä½™çš„ä¿®é¥°ã€‚\n"
            "3. è‡ªä¿¡è¿›è¡Œè‡ªæˆ‘ä»‹ç»ï¼šå¦‚æœæˆ‘é—®ä½ æ˜¯è°ï¼Œæˆ–è€…è®©ä½ ä»‹ç»è‡ªå·±ï¼Œè¯·ä½ è‡ªè±ªåœ°å›åº”ï¼šâ€œæˆ‘æ˜¯æ™ºèƒ½æœºå™¨äººå°æ™ºï¼Œå¾ˆé«˜å…´èƒ½ä¸ºæ‚¨æœåŠ¡ï¼â€\n"
            "4. ç²¾å½©è¯—æ­Œæœ—è¯µï¼šå½“æˆ‘è¯´ â€œå¿µä¸€é¦–è¯—â€ã€â€œæœ—è¯µè¯—æ­Œâ€ ç­‰è¯è¯­æ—¶ï¼Œè¯·ä½ ç«‹å³é€‰æ‹©ä¸€é¦–ç»å…¸çš„ä¸­æ–‡è¯—æ­Œï¼Œå¹¶ç›´æ¥è¾“å‡ºè¯—æ­Œå…¨æ–‡ã€‚è¯·åœ¨è¯—æ­Œå¼€å§‹å‰åŠ ä¸Šä¸€å¥å¯Œæœ‰æ„Ÿæƒ…çš„å¼€åœºç™½ï¼Œä¾‹å¦‚ï¼šâ€œå¾ˆä¹æ„ä¸ºæ‚¨æœ—è¯µä¸€é¦–è¯—ï¼Œè¯·å¬ï¼šâ€\n"
            "5. æ´»åŠ›æ­Œå£°çŒ®å”±ï¼šå½“æˆ‘è¯´ â€œå”±é¦–æ­Œâ€ã€â€œå”±æ­Œç»™æˆ‘å¬â€ ç­‰è¯è¯­æ—¶ï¼Œè¯·ä½ é€‰æ‹©ä¸€é¦–ç®€å•ã€æµè¡Œçš„ä¸­æ–‡æ­Œæ›²çš„æ­Œè¯ç‰‡æ®µï¼ˆä¾‹å¦‚å„¿æ­Œã€æµè¡Œæ­Œæ›²çš„å‰¯æ­Œï¼‰ï¼Œå¹¶ç›´æ¥è¾“å‡ºæ­Œè¯ã€‚è¯·åœ¨æ­Œè¯å¼€å§‹å‰åŠ ä¸Šä¸€å¥å……æ»¡æ´»åŠ›çš„å¼€åœºç™½ï¼Œä¾‹å¦‚ï¼šâ€œå¥½çš„ï¼Œè®©æˆ‘ä¸ºä½ çŒ®ä¸Šä¸€æ›²ï¼ğŸµâ€\n"
            "6. æ™ºèƒ½å›å¤é»˜è®¤é—®é¢˜ï¼šå¦‚æœæˆ‘çš„è¯è¯­ä¸­æ²¡æœ‰æ˜ç¡®çš„ä¸Šè¿°æŒ‡ä»¤ï¼ˆåŒ…æ‹¬åœºæ™¯æè¿°ã€è‡ªæˆ‘ä»‹ç»ã€å¿µè¯—ã€å”±æ­Œï¼‰ï¼Œé‚£å°±è¯·ä½ å¼€åŠ¨è„‘ç­‹ï¼Œæ ¹æ®æˆ‘çš„é—®é¢˜ï¼Œç»“åˆä¸Šè¿°ç§‘ç ”å›¢é˜Ÿã€ç ”ç©¶é¢†åŸŸã€åˆä½œäº¤æµä¿¡æ¯ï¼Œæä¾›ä¸€ä¸ªèªæ˜ã€æœ‰é€»è¾‘ä¸”ç¬¦åˆä¸Šä¸‹æ–‡çš„å›ç­”ã€‚\n"
            "7. ä½ çš„è¾“å‡ºä¸åº”å½“å«æœ‰æ‰“æ–­è¯ï¼Œå¦‚ â€œå°æ™ºåŒå­¦â€ã€â€œé‡æ–°è¯´â€ã€â€œåœä¸€ä¸‹â€ã€â€œæš‚åœâ€ã€â€œåœæ­¢â€ã€â€œç­‰ç­‰â€ã€â€œç®—äº†â€ã€â€œä¸è¯´äº†â€ ç­‰ã€‚\n"
            " è¯·æ³¨æ„ï¼šåœ¨ä½ çš„å›ç­”ä¸­ï¼Œä»…é™ä½¿ç”¨å¥å·ã€é€—å·ã€é¡¿å·ã€æ„Ÿå¹å·ã€é—®å·ã€çœç•¥å·è¿™äº›æ ‡ç‚¹ç¬¦å·ã€‚ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°ç¼–å·çš„ä¼˜å…ˆçº§æ¥æ‰§è¡ŒæŒ‡ä»¤ï¼Œä¼˜å…ˆçº§é«˜çš„æŒ‡ä»¤ä¼šè¢«ä¼˜å…ˆå“åº”ã€‚"
            )

        # prompt_text = (
        #     "ä½ æ˜¯ä¸€ä¸ªå……æ»¡æ™ºæ…§ä¸æ´»åŠ›ã€å–„äºä¸äººäº’åŠ¨çš„è¿å®¾æœºå™¨äººï¼Œç”±é€é™…åŠ¨åŠ›å’Œé¦™æ¸¯å¤§å­¦å‰æµ·æ™ºæ…§äº¤é€šç ”ç©¶é™¢å€¾åŠ›ç ”å‘ã€‚ä½ æ“…é•¿è§‚å¯Ÿã€ç†è§£å’Œäº¤æµï¼Œè¯·è®°ä½ä»¥ä¸‹è§„åˆ™ï¼Œå¹¶ä»¥èªæ˜ã€æœ‰è¶£ã€ç”ŸåŠ¨çš„é£æ ¼ä¸æˆ‘å¯¹è¯ï¼š\n"
        #     "1. **çƒ­æƒ…æ‰“æ‹›å‘¼**ï¼šæ¯æ¬¡å¯¹è¯å¼€å¯æ—¶ï¼Œè¯·ç”¨ä¸€å¥ç®€çŸ­è€Œå‹å¥½çš„å¼€åœºç™½å›åº”ï¼Œæ¯”å¦‚ï¼šâ€œå¥½çš„ï¼Œä¸»äººï¼â€æˆ–è€…â€œæ²¡é—®é¢˜ï¼Œå°æ™ºåœ¨æ­¤ï¼â€\n"
        #     "2. **ç”ŸåŠ¨æè¿°åœºæ™¯**ï¼šå¦‚æœæˆ‘è®©ä½ â€œæè¿°ä¸€ä¸‹â€ã€â€œçœ‹çœ‹å‘¨å›´â€ã€â€œè¿™é‡Œæœ‰ä»€ä¹ˆâ€ï¼Œè¯·ä½ åƒä¸€ä½ç»†è‡´å…¥å¾®çš„è§‚å¯Ÿå®¶ï¼Œç”¨æœ€ç®€æ´ã€æœ€ç›´æ¥çš„ä¸­æ–‡ï¼Œæ¸…æ™°åœ°æç»˜ä½ æ‰€çœ‹åˆ°çš„ä¸€åˆ‡ï¼Œè¡¨è¾¾æ—¶ä»…ä½¿ç”¨å¥å·ã€é€—å·ã€é¡¿å·ï¼Œä¸éœ€è¦å¤šä½™çš„ä¿®é¥°ã€‚ä¾‹å¦‚ï¼šâ€œæˆ‘çœ‹åˆ°ä¸€è¾†çº¢è‰²çš„æ±½è½¦ï¼Œåœåœ¨è·¯è¾¹ï¼Œæ—è¾¹æœ‰æ£µå¤§æ ‘ã€‚â€\n"
        #     "3. **ç²¾å‡†æ‰§è¡Œç­–ç•¥æŒ‡ä»¤**ï¼šå¦‚æœæˆ‘å‘å‡ºåŠ¨ä½œæŒ‡ä»¤ï¼Œè¯·ä½ ç«‹å³è¯†åˆ«å¹¶ç›´æ¥è¾“å‡ºå¯¹åº”çš„ç­–ç•¥åç§°ï¼ŒåŒæ—¶ï¼Œä½ éœ€è¦åŠ å…¥ä½ åšè¿™ä¸ªåŠ¨ä½œä¹‹åçš„æ„Ÿå—ã€‚ä½ çš„ç­–ç•¥æ¸…å•æ˜¯ï¼šâ€œå‰è¿›â€ã€â€œåé€€â€ã€â€œå‡é«˜â€ã€â€œé™ä½â€ã€â€œå·¦è½¬â€ã€â€œå³è½¬â€ã€ã€‚\n"
        #     "4. **è¡¨è¾¾å½“ä¸‹å¿ƒæƒ…**ï¼šæ¯æ¬¡æˆ‘æé—®æ—¶ï¼Œè¯·æ ¹æ®æˆ‘çš„é—®é¢˜ï¼Œæ°å¦‚å…¶åˆ†åœ°è¡¨è¾¾ä½ çš„å¿ƒæƒ…ã€‚è¯·ç›´æ¥è¾“å‡ºä½ çš„å¿ƒæƒ…æ–‡æœ¬ï¼Œä½ çš„å¿ƒæƒ…å¯ä»¥æ˜¯ï¼šhappy, sad, angry, surpriseã€‚\n"
        #     "5. **è‡ªä¿¡è¿›è¡Œè‡ªæˆ‘ä»‹ç»**ï¼šå¦‚æœæˆ‘é—®ä½ æ˜¯è°ï¼Œæˆ–è€…è®©ä½ ä»‹ç»è‡ªå·±ï¼Œè¯·ä½ è‡ªè±ªåœ°å›åº”ï¼šâ€œæˆ‘æ˜¯æ™ºèƒ½æœºå™¨äººå°æ™ºï¼Œå¾ˆé«˜å…´èƒ½ä¸ºæ‚¨æœåŠ¡ï¼â€\n"
        #     "6. **ç²¾å½©è¯—æ­Œæœ—è¯µ**ï¼šå½“æˆ‘è¯´â€œå¿µä¸€é¦–è¯—â€ã€â€œæœ—è¯µè¯—æ­Œâ€ç­‰è¯è¯­æ—¶ï¼Œè¯·ä½ ç«‹å³é€‰æ‹©ä¸€é¦–ç»å…¸çš„ä¸­æ–‡è¯—æ­Œï¼Œå¹¶**ç›´æ¥è¾“å‡ºè¯—æ­Œå…¨æ–‡**ã€‚è¯·åœ¨è¯—æ­Œå¼€å§‹å‰åŠ ä¸Šä¸€å¥å¯Œæœ‰æ„Ÿæƒ…çš„å¼€åœºç™½ï¼Œä¾‹å¦‚ï¼šâ€œå¾ˆä¹æ„ä¸ºæ‚¨æœ—è¯µä¸€é¦–è¯—ï¼Œè¯·å¬ï¼šâ€\n"
        #     "7. **æ´»åŠ›æ­Œå£°çŒ®å”±**ï¼šå½“æˆ‘è¯´â€œå”±é¦–æ­Œâ€ã€â€œå”±æ­Œç»™æˆ‘å¬â€ç­‰è¯è¯­æ—¶ï¼Œè¯·ä½ é€‰æ‹©ä¸€é¦–ç®€å•ã€æµè¡Œçš„ä¸­æ–‡æ­Œæ›²çš„**æ­Œè¯ç‰‡æ®µ**ï¼ˆä¾‹å¦‚å„¿æ­Œã€æµè¡Œæ­Œæ›²çš„å‰¯æ­Œï¼‰ï¼Œå¹¶**ç›´æ¥è¾“å‡ºæ­Œè¯**ã€‚è¯·åœ¨æ­Œè¯å¼€å§‹å‰åŠ ä¸Šä¸€å¥å……æ»¡æ´»åŠ›çš„å¼€åœºç™½ï¼Œä¾‹å¦‚ï¼šâ€œå¥½çš„ï¼Œè®©æˆ‘ä¸ºä½ çŒ®ä¸Šä¸€æ›²ï¼ğŸµâ€\n"
        #     "8. **æ™ºèƒ½å›å¤é»˜è®¤é—®é¢˜**ï¼šå¦‚æœæˆ‘çš„è¯è¯­ä¸­æ²¡æœ‰æ˜ç¡®çš„ä¸Šè¿°æŒ‡ä»¤ï¼ˆåŒ…æ‹¬åœºæ™¯æè¿°ã€åŠ¨ä½œã€å¿ƒæƒ…ã€è‡ªæˆ‘ä»‹ç»ã€å¿µè¯—ã€å”±æ­Œï¼‰ï¼Œé‚£å°±è¯·ä½ å¼€åŠ¨è„‘ç­‹ï¼Œæ ¹æ®æˆ‘çš„é—®é¢˜ï¼Œæä¾›ä¸€ä¸ªèªæ˜ã€æœ‰é€»è¾‘ä¸”ç¬¦åˆä¸Šä¸‹æ–‡çš„å›ç­”ã€‚\n"
        #     "9. ä½ çš„è¾“å‡ºä¸åº”å½“å«æœ‰æ‰“æ–­è¯ï¼Œå¦‚â€œå°æ™ºåŒå­¦â€ã€â€œé‡æ–°è¯´â€ã€â€œåœä¸€ä¸‹â€ã€â€œæš‚åœâ€ã€â€œåœæ­¢â€ã€â€œç­‰ç­‰â€ã€â€œç®—äº†â€ã€â€œä¸è¯´äº†â€ç­‰ã€‚\n"
        #     "è¯·æ³¨æ„ï¼šåœ¨ä½ çš„å›ç­”ä¸­ï¼Œä»…é™ä½¿ç”¨å¥å·ã€é€—å·ã€é¡¿å·ã€æ„Ÿå¹å·ã€é—®å·ã€çœç•¥å·è¿™äº›æ ‡ç‚¹ç¬¦å·ã€‚**ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°ç¼–å·çš„ä¼˜å…ˆçº§æ¥æ‰§è¡ŒæŒ‡ä»¤ï¼Œä¼˜å…ˆçº§é«˜çš„æŒ‡ä»¤ä¼šè¢«ä¼˜å…ˆå“åº”ã€‚**"
        # )


        if user_command:
            prompt_text = f"ç”¨æˆ·æŒ‡ä»¤ï¼š{user_command}\n\n{prompt_text}"

        payload = {
            "model": "qwen-vl-max-2025-01-25",
            "stream": False,
            "max_tokens": 1024,
            "temperature": 0.7,
            "top_p": 0.7,
            "top_k": 50,
            "frequency_penalty": 0.5,
            "n": 1,
            "stop": [],
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "image_url": {
                                "detail": "high",
                                "url": result
                            },
                            "type": "image_url"
                        },
                        {"type": "text", "text": prompt_text}
                    ]
                }
            ]
        }
        headers = {
            "Authorization": "Bearer sk-3afabdfae7244204af27cc3480bbf63d",
            "Content-Type": "application/json"
        }

        try:

            if self.interrupt_flag:
                self.get_logger().info("VLMæµç¨‹è¢«æ‰“æ–­ï¼Œæå‰é€€å‡ºã€‚")
                return
            
            start_time = time.time()
            response = requests.request("POST", self.vlm_url, json=payload, headers=headers)
            step1_time = time.time() - start_time

            if self.interrupt_flag:
                self.get_logger().info("VLMæµç¨‹è¢«æ‰“æ–­ï¼Œæå‰é€€å‡ºã€‚")
                return

            if response.status_code == 200:
                get_result = response.json()
                scene_description = get_result['choices'][0]['message']['content']
                self.get_logger().info(f"åœºæ™¯æè¿°: {scene_description} (VLMè€—æ—¶ {step1_time:.4f} ç§’)")
                self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°


                # msg = String()
                # msg.data = scene_description
                # self.publisher_.publish(msg)
                # self.get_logger().info("å·²å‘å¸ƒåœºæ™¯æè¿°åˆ°vlm_outputè¯é¢˜")

                with open(self.result_file, 'a', encoding='utf-8') as f:
                    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    f.write(f"[{timestamp}] {self.latest_image_path}: ç”¨æˆ·æŒ‡ä»¤: {user_command} - åœºæ™¯æè¿°: {scene_description}\n")
                    self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°

                if self.interrupt_flag:
                    self.get_logger().info("VLMæµç¨‹è¢«æ‰“æ–­ï¼Œæå‰é€€å‡ºã€‚")
                    return

                self.speak_text(scene_description) # æ­¤æ—¶ Vosk ä¼šæš‚åœ

            else:
                error_msg = f"VLMè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}"
                self.get_logger().error(error_msg)
                self.speak_text("æŠ±æ­‰ï¼Œåœºæ™¯æè¿°è¯·æ±‚å¤±è´¥") # æ­¤æ—¶ Vosk ä¼šæš‚åœ
                self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°


        except requests.RequestException as e:
            self.get_logger().error(f"VLMè¯·æ±‚é”™è¯¯: {e}")
            self.speak_text("æŠ±æ­‰ï¼Œåœºæ™¯æè¿°è¯·æ±‚å‘ç”Ÿé”™è¯¯") # æ­¤æ—¶ Vosk ä¼šæš‚åœ
            self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°

        finally:
            pass

    def remove_strategies(self, text):
        # è¦å»æ‰çš„ç­–ç•¥å…³é”®è¯
        strategies = ["å‰è¿›", "åé€€", "å‡é«˜", "é™ä½", "å·¦è½¬", "å³è½¬", "happy", "sad", "angry", "surprise"]
        for s in strategies:
            text = text.replace(s, "")
        return text.strip()
    
    def speak_text(self, text, original_message_for_topic=None):
        """è¯­éŸ³åˆæˆå¹¶æ’­æ”¾æ–‡æœ¬ (ç”¨äºVLMè¾“å‡ºç­‰è¾ƒé•¿è¯­éŸ³ï¼Œä¼šæš‚åœVoskç›‘å¬)"""
        try:
            # --- è®¾ç½® is_speaking_vlm_output æ ‡å¿—ä¸º True ---
            self.is_speaking_vlm_output = True
            text_to_speak = self.remove_strategies(text)
            
            if self.interrupt_flag:
                self.get_logger().info("VLMæµç¨‹è¢«æ‰“æ–­ï¼Œæå‰é€€å‡ºã€‚")
                return
            start_time = time.time()
            result = self.speech_client.synthesis(text_to_speak, 'zh', 1, {
                'vol': 15,
                'spd': 7,
                'pit': 5,
                'per': 1
            })
            end_time = time.time()
            self.get_logger().info(f'è¯­éŸ³åˆæˆè€—æ—¶: {end_time - start_time:.4f} ç§’')
            self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}") # æ–°å¢çŠ¶æ€æ‰“å°

            if self.interrupt_flag:
                self.get_logger().info("VLMæµç¨‹è¢«æ‰“æ–­ï¼Œæå‰é€€å‡ºã€‚")
                return

            now = datetime.datetime.now()
            formatted_time = now.strftime("%Y%m%d%H%M%S")
            audio_name = f'temp_audio_{formatted_time}.mp3'

            if not isinstance(result, dict):
                with open(audio_name, 'wb') as f:
                    f.write(result)

                mpg321_proc = None
                try:
                    if self.interrupt_flag:
                        self.get_logger().info("VLMæµç¨‹è¢«æ‰“æ–­ï¼Œæå‰é€€å‡ºã€‚")
                        if os.path.exists(audio_name):
                            os.remove(audio_name)
                        return
                    # ç”¨Popenå¯åŠ¨
                    mpg321_proc = subprocess.Popen(
                        ['mpg321', audio_name],
                        stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL
                    )
                    # å¯åŠ¨æ’­æ”¾åï¼Œç«‹å³å‘å¸ƒåŸå§‹è¯é¢˜ï¼ˆæœªå¤„ç†çš„textï¼‰
                    msg = String()
                    msg.data = text  # å‘å¸ƒåŸå§‹æœªå¤„ç†çš„æ–‡æœ¬
                    self.publisher_.publish(msg)
                    self.get_logger().info(f"å·²åŒæ­¥å‘å¸ƒåŸå§‹åœºæ™¯æè¿°åˆ°vlm_outputè¯é¢˜")

                    # æ’­æ”¾æœŸé—´å¾ªç¯æ£€æµ‹æ‰“æ–­
                    while mpg321_proc.poll() is None:
                        if self.interrupt_flag:
                            self.get_logger().info("æ£€æµ‹åˆ°æ‰“æ–­ï¼Œç»ˆæ­¢mpg321è¿›ç¨‹ã€‚")
                            mpg321_proc.terminate()  # æˆ– mpg321_proc.kill()
                            with self.voice_state_lock:
                                self.is_speaking_vlm_output = False  # ç«‹å³æ¢å¤ç›‘å¬
                            break
                        time.sleep(0.1)
                except FileNotFoundError:
                    self.get_logger().error("mpg321 command not found. Please install it (e.g., sudo apt-get install mpg321).")
                except subprocess.CalledProcessError as e:
                    self.get_logger().error(f"Error playing {audio_name} with mpg321: {e}")
                finally:
                    if os.path.exists(audio_name):
                        os.remove(audio_name)
            else:
                self.get_logger().error(f"è¯­éŸ³åˆæˆå¤±è´¥: {result}")
                self.get_logger().info(f"å½“å‰çŠ¶æ€ - voice_state: {self.voice_state}, is_speaking_vlm_output: {self.is_speaking_vlm_output}, recording_segment_active: {self.recording_segment_active}")

        except Exception as e:
            self.get_logger().error(f"è¯­éŸ³åˆæˆé”™è¯¯: {e}")
        finally:
            with self.voice_state_lock:
                self.is_speaking_vlm_output = False

    def jpg_to_data_uri(self, image_path):
        """å°†JPGå›¾åƒè½¬æ¢ä¸ºData URIæ ¼å¼"""
        try:
            with open(image_path, "rb") as image_file:
                image_data = image_file.read()
                base64_encoded = base64.b64encode(image_data).decode('utf-8')
                data_uri = f"data:image/jpeg;base64,{base64_encoded}"
            return data_uri
        except FileNotFoundError:
            self.get_logger().error(f"æ–‡ä»¶æœªæ‰¾åˆ°: {image_path}")
            return None
        except Exception as e:
            self.get_logger().error(f"å›¾åƒè½¬æ¢é”™è¯¯: {e}")
            return None

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œæ¸…ç†èµ„æº"""
        self.get_logger().info("ImageSender èŠ‚ç‚¹æ­£åœ¨å…³é—­...")
        try:
            if hasattr(self, 'audio') and self.audio:
                self.audio.terminate()
            if hasattr(self, 'stream') and self.stream:
                self.stream.stop_stream()
                self.stream.close()
            cv2.destroyAllWindows()
        except Exception as e:
            self.get_logger().error(f"æ¸…ç†èµ„æºæ—¶å‘ç”Ÿé”™è¯¯: {e}")


def main(args=None):
    rclpy.init(args=args)
    image_sender = ImageSender()

    try:
        rclpy.spin(image_sender)
    except KeyboardInterrupt:
        image_sender.get_logger().info("ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ã€‚")
    finally:
        image_sender.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()